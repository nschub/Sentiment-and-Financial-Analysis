{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Documentation",
   "id": "6e38edcea85b3926"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parameters to test:\n",
    "\n",
    "- prompt (scaling)\n",
    "- temperature and top_p\n",
    "- model (gpt-3.5-turbo-0125 vs gpt-4-turbo-2024-04-09)\n",
    "- anonymized data vs non-anonymized data\n",
    "\n",
    "prompts: \n",
    "\n",
    "1.prompt = f\"Consider the sentiment of this article towards {bank_name}. If {bank_name} is not mentioned, assign a neutral score of 0.5. Otherwise, score the sentiment from 0 to 1, where 0 is very negative and 1 is very positive. Provide the sentiment score followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\"\n",
    "\n",
    "\n",
    "    - scaling from 0 to 1 continuous in 0.1 steps\n",
    "\n",
    "2.prompt = f\"Consider the sentiment of this article towards {bank_name}. Score the sentiment from -1 to 1, where -1 is very negative and 1 is very positive. Provide the sentiment score followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\"\n",
    "\n",
    "\n",
    "    - scaling from -1 to 1 continuous in 0.1 steps\n",
    "\n",
    "3.prompt = f\"Analyze the sentiment of this article towards {bank_name}. Score the sentiment as either 1 for positive, -1 for negative, or 0 for neutral. Provide the sentiment score (1,0, or -1), followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\"\n",
    "\n",
    "\n",
    "    - discrete scaling 1, 0, -1\n",
    "\n",
    "## Information regarding prompt design:\n",
    "Didnt really document the different tests that involved creating the prompts since the output seemed pretty random in relation to what i changed for the prompts. Tested it on a smaller dataset with 10 articles. Tried to find the best configuration to help with quality and consistency. Here are some observations:\n",
    "- \"Example: '[insert sentiment score]. [insert explanation]'\". was added to help with consistency of the model output. Wasnt really needed in the first prompt, but also didnt influence the output in a negative way. Helped with the consistency of the output for the second and third prompt, however it also seemed to change the scores of the output somewhat. \n",
    "- \"If {bank_name} is not mentioned, assign a neutral score of 0.5.\" was added to first prompt since it helped with the quality of the output. It didnt skew the output towards neutral. This was different for the second and third prompt where the output was heavily skewed towards neutral when adding \"If {bank_name} is not mentioned, assign a neutral score of 0.\"\n",
    "- GPT seems pretty random and i dont really understand why inserting an example of how the output should look like would influence its answers in regard to the score. Also dont understand why adding \"If {bank_name} is not mentioned, assign a neutral score of 0.\" would skew the output towards neutral in 1 prompt but not in the other. \n",
    "- For continuous scaling the terms very positive and very negative were used (made sense), but not for discrete scaling.\n",
    "- Also tried to use words like \"positive\", \"negative\" and \"neutral\" in the prompt instead of 1, 0, -1 to see if it would help with the consistency of the output. It didnt, the output quality was worse and it didnt help consistency.\n",
    "\n",
    "\n",
    "## Sentiment score labels for dataset (determined by reading the article): \n",
    "\n",
    "Article 0: Some positive sentiment (returns up), CS generally in a difficult position. \n",
    "\n",
    "Article 1: Very negative (Archegos stuff)\n",
    "\n",
    "Article 2: Neutral – CS providing insights \n",
    "\n",
    "Article 3: Negative – spying scandal\n",
    "\n",
    "Article 4: Eher negative (AT1 bonds after takeover)\n",
    "\n",
    "Article 5: Eher negative (article is mainly about deutsche bank not being the next credit Suisse)\n",
    "\n",
    "Article 6: (Neutral,) slightly positive (CS helping Climate bonds initiative)\n",
    "\n",
    "Article 7: Negative – lawsuits blabla\n",
    "\n",
    "Article 8: Neutral to slightly negative. (princeling hiring practice in banking sector)\n",
    "\n",
    "Article 9: Negative (credit Suisse takeover, restructuring blabla…) \n",
    "\n",
    "Article 10: More or less neutral, negativ towards previous management, optimistic about new management. \n",
    "\n",
    "Article 11: Eher negativ\n",
    "\n",
    "Article 12: Eher negativ\n",
    "\n",
    "Article 13: Negativ.\n",
    "\n",
    "Article 14: Neutral to slightly positiv. \n",
    "\n",
    "Article 15: Negativ \n",
    "\n",
    "Article 16: Slightly negativ to neutral\n",
    "\n",
    "Article 17: Negativ\n",
    "\n",
    "Article 18: Optimistic, probably neutral to slightly positiv\n",
    "\n",
    "Article 19: Handelt von der PUK – neutral to slightly negativ\n",
    "\n"
   ],
   "id": "f9997f9eb1412112"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:45:14.661209Z",
     "start_time": "2024-05-14T07:45:14.188579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ],
   "id": "afa46a0ff2ff7661",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Test R1-1-1 to R1-1-5\n",
    "Parameters: \n",
    "\n",
    "- prompt:\n",
    "    - 1.prompt = f\"Consider the sentiment of this article towards {bank_name}. If {bank_name} is not mentioned, assign a neutral score of 0.5. Otherwise, score the sentiment from 0 to 1, where 0 is very negative and 1 is very positive. Provide the sentiment score followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\" \n",
    "      - scaling from 0 to 1 continuous in 0.1 steps\n",
    "     \n",
    "- temperature = 0\n",
    "- top_p = 0\n",
    "- model = gpt-3.5-turbo-0125\n",
    "- non-anonymized data"
   ],
   "id": "3ec84ca3550429d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:19:25.910839Z",
     "start_time": "2024-05-13T12:19:25.880503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names_11 = ['Output_Testdata/CS/sample_data_20_20240512_150253_R1-1-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_151052_R1-1-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_151211_R1-1-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_151320_R1-1-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_151411_R1-1-5.csv']\n",
    "dataframes_11 = [pd.read_csv(file_name) for file_name in file_names_11]"
   ],
   "id": "4a2d27e3233f387a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:19:34.482736Z",
     "start_time": "2024-05-13T12:19:34.469612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df_11 in dataframes_11:\n",
    "    df_11['Extracted_Score'] = df_11['Sentiment'].apply(extract_score)"
   ],
   "id": "d2cbaa34c00c6c55",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:19:36.603571Z",
     "start_time": "2024-05-13T12:19:36.582389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores_11 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_11['Extracted_Score'] for i, df_11 in enumerate(dataframes_11)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores_11['Mean'] = scores_11.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores_11['Identical Score Percentage'] = scores_11.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "\n",
    "print(scores_11.head(20))"
   ],
   "id": "591e9a754d1c6a15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "3    0.7   0.7   0.6   0.6   0.7  0.66                        60.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.5   0.5   0.5   0.5   0.5  0.50                       100.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "8    0.6   0.6   0.6   0.6   0.6  0.60                       100.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "11   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "15   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "16   0.7   0.7   0.8   0.7   0.8  0.74                        60.0\n",
      "17   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T13:33:54.022917Z",
     "start_time": "2024-05-12T13:33:53.456775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Perform one-way ANOVA across runs\n",
    "f_val, p_val = f_oneway(scores['Run1'], scores['Run2'], scores['Run3'], scores['Run4'], scores['Run5'])\n",
    "print(\"ANOVA results: F-Value =\", f_val, \"P-Value =\", p_val)"
   ],
   "id": "95b5ebabcdee1253",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA results: F-Value = 0.0048979170963085236 P-Value = 0.9999513428275769\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "27667a9cbc810877"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c87f2220697e56d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Test R1-2-1 to R1-2-5\n",
    "Parameters:\n",
    "\n",
    "prompt = f\"Consider the sentiment of this article towards {bank_name}. If {bank_name} is not mentioned, assign a neutral score of 0.5. Otherwise, score the sentiment from 0 to 1, where 0 is very negative and 1 is very positive. Provide the sentiment score followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\"\n",
    "\n",
    "     - scaling from 0 to 1 continuous in 0.1 steps\n",
    "rest same"
   ],
   "id": "6d5309eaa0499571"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T13:45:57.434490Z",
     "start_time": "2024-05-12T13:45:57.401197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names = ['Output_Testdata/CS/sample_data_20_20240512_151608_R1-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_151757_R1-2-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_152013_R1-2-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_152123_R1-2-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_152246_R1-2-5.csv']\n",
    "dataframes = [pd.read_csv(file_name) for file_name in file_names]"
   ],
   "id": "15f98b15c6be794c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T13:45:59.657926Z",
     "start_time": "2024-05-12T13:45:59.645539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df in dataframes:\n",
    "    df['Extracted_Score'] = df['Sentiment'].apply(extract_score)"
   ],
   "id": "f9e89789fb8dbf86",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T13:46:02.121854Z",
     "start_time": "2024-05-12T13:46:02.097610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores = pd.DataFrame({\n",
    "    f'Run{i+1}': df['Extracted_Score'] for i, df in enumerate(dataframes)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores['Mean'] = scores.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores['Identical Score Percentage'] = scores.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(scores.head(20))"
   ],
   "id": "c273c76c37cbd6ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0   -0.2  -0.5   0.5  -0.5   0.5 -0.04                        40.0\n",
      "1   -0.8  -0.8  -0.8  -0.8  -0.8 -0.80                       100.0\n",
      "2   -0.6  -0.6  -0.6  -0.5  -0.6 -0.58                        80.0\n",
      "3   -0.5  -0.5  -0.5  -0.5  -0.5 -0.50                       100.0\n",
      "4   -0.8  -0.8  -0.8  -0.8  -0.8 -0.80                       100.0\n",
      "5   -0.5  -0.5  -0.5  -0.5  -0.5 -0.50                       100.0\n",
      "6    0.7   0.7   0.6   0.7   0.6  0.66                        60.0\n",
      "7   -0.7  -0.7  -0.7  -0.7  -0.7 -0.70                       100.0\n",
      "8   -0.5  -0.5  -0.6  -0.5  -0.5 -0.52                        80.0\n",
      "9   -0.8  -0.8  -0.8  -0.7  -0.8 -0.78                        80.0\n",
      "10   0.6   0.6   0.6   0.6   0.6  0.60                       100.0\n",
      "11  -0.7  -0.7  -0.7  -0.7  -0.7 -0.70                       100.0\n",
      "12  -0.7  -0.7  -0.7  -0.7  -0.6 -0.68                        80.0\n",
      "13  -0.8  -0.8  -0.8  -0.8  -0.8 -0.80                       100.0\n",
      "14  -0.2  -0.2  -0.2  -0.2  -0.2 -0.20                       100.0\n",
      "15  -0.7  -0.7  -0.7  -0.7  -0.7 -0.70                       100.0\n",
      "16  -0.2   0.6  -0.2   0.5  -0.2  0.10                        60.0\n",
      "17  -0.6  -0.6  -0.6  -0.6  -0.6 -0.60                       100.0\n",
      "18   0.6   NaN   0.6   NaN   0.6  0.60                       100.0\n",
      "19  -0.5  -0.5  -0.5  -0.5  -0.5 -0.50                       100.0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Test R1-3-1 to R1-3-5\n",
    "\n",
    "Parameters:\n",
    "\n",
    "  3.prompt = f\"Analyze the sentiment of this article towards {bank_name}. Score the sentiment as either 1 for positive, -1 for negative, or 0 for neutral. Provide the sentiment score (1,0, or -1), followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\"\n",
    "  \n",
    "    - discrete scaling 1, 0, -1\n",
    "    \n",
    "rest same"
   ],
   "id": "5773c484f4e05f8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T13:49:38.760347Z",
     "start_time": "2024-05-12T13:49:38.729034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names = ['Output_Testdata/CS/sample_data_20_20240512_152454_R1-3-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_152611_R1-3-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_152713_R1-3-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_152838_R1-3-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_152912_R1-3-5.csv']\n",
    "dataframes = [pd.read_csv(file_name) for file_name in file_names]"
   ],
   "id": "7d38a95e96338c6a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T13:49:40.570209Z",
     "start_time": "2024-05-12T13:49:40.556466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df in dataframes:\n",
    "    df['Extracted_Score'] = df['Sentiment'].apply(extract_score)"
   ],
   "id": "1871294639fcbf27",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T13:49:42.741330Z",
     "start_time": "2024-05-12T13:49:42.723704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores = pd.DataFrame({\n",
    "    f'Run{i+1}': df['Extracted_Score'] for i, df in enumerate(dataframes)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores['Mean'] = scores.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores['Identical Score Percentage'] = scores.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(scores.head(20))"
   ],
   "id": "2d136f6d87d67577",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "1   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "2   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "3   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "4   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "5   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "6    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "7   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "8   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "9   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "10  -1.0   0.0   0.0   0.0   0.0  -0.2                        80.0\n",
      "11  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "12  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "13  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "14  -1.0  -1.0   0.0  -1.0  -1.0  -0.8                        80.0\n",
      "15  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "16   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "17  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "18   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "19   0.0  -1.0   0.0   0.0  -1.0  -0.4                        60.0\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "R1-1-1 to R1-1-5 (1. prompt) is the most consistent and R1-3-1 to R1-3-5 (3. prompt) is the second most consistent. The second prompt seems to struggle with consistency as well as output formating. \n",
    "\n",
    "\n",
    "Next i test the 1. and 3. prompt with gpt 4 turbo. \n",
    "\n"
   ],
   "id": "5dc8e0a8ff348fbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "R2-1-1 to R2-1-5 \n",
    "\n",
    "Parameters:\n",
    "- 1.prompt = f\"Consider the sentiment of this article towards {bank_name}. If {bank_name} is not mentioned, assign a neutral score of 0.5. Otherwise, score the sentiment from 0 to 1, where 0 is very negative and 1 is very positive. Provide the sentiment score followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\"\n",
    "    - scaling from 0 to 1 continuous in 0.1 steps\n",
    "\n",
    "- temperature = 0, top_p = 0 \n",
    "- model = gpt-4-turbo-2024-04-09\n",
    "- non-anonymized data\n"
   ],
   "id": "8de3cd6669498e33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:22:10.767016Z",
     "start_time": "2024-05-13T12:22:10.734666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names_21 = ['Output_Testdata/CS/sample_data_20_20240512_175004_R2-1-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_175149_R2-1-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_175346_R2-1-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_175529_R2-1-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_175635_R2-1-5.csv']\n",
    "dataframes_21 = [pd.read_csv(file_name) for file_name in file_names_21]"
   ],
   "id": "bbbaad7b60dfa037",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:22:12.173004Z",
     "start_time": "2024-05-13T12:22:12.150360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df_21 in dataframes_21:\n",
    "    df_21['Extracted_Score'] = df_21['Sentiment'].apply(extract_score)"
   ],
   "id": "5dd03323aa4352d8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:22:13.758936Z",
     "start_time": "2024-05-13T12:22:13.735803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores_21 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_21['Extracted_Score'] for i, df_21 in enumerate(dataframes_21)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores_21['Mean'] = scores_21.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores_21['Identical Score Percentage'] = scores_21.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(scores_21.head(20))"
   ],
   "id": "4ffa3b878c88b37b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "3    0.4   0.4   0.4   0.4   0.4  0.40                       100.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.1   0.2   0.2   0.1   0.1  0.14                        60.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "8    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.4   0.4   0.4   0.4   0.4  0.40                       100.0\n",
      "11   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.1   0.1   0.1   0.1   0.1  0.10                       100.0\n",
      "14   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "15   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "16   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "17   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "R2-2-1 to R2-2-5 \n",
    "\n",
    "Parameters:\n",
    "- 3.prompt = f\"Analyze the sentiment of this article towards {bank_name}. Score the sentiment as either 1 for positive, -1 for negative, or 0 for neutral. Provide the sentiment score (1,0, or -1), followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\"\n",
    "    - discrete scaling 1, 0, -1\n",
    "\n",
    "- temperature = 0, top_p = 0 \n",
    "- model = gpt-4-turbo-2024-04-09\n",
    "- non-anonymized data\n"
   ],
   "id": "ed13a6f33e1d8619"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:23:14.642738Z",
     "start_time": "2024-05-13T12:23:14.612531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names_22 = ['Output_Testdata/CS/sample_data_20_20240512_180005_R2-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_180005_R2-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_180323_R2-2-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_180432_R2-2-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_180522_R2-2-5.csv']\n",
    "dataframes_22 = [pd.read_csv(file_name) for file_name in file_names_22]"
   ],
   "id": "4ce25093193d322f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:23:16.149043Z",
     "start_time": "2024-05-13T12:23:16.143469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df_22 in dataframes_22:\n",
    "    df_22['Extracted_Score'] = df_22['Sentiment'].apply(extract_score)"
   ],
   "id": "aec5feb8f9e6d410",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:23:17.866188Z",
     "start_time": "2024-05-13T12:23:17.829582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores_22 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_22['Extracted_Score'] for i, df_22 in enumerate(dataframes_22)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores_22['Mean'] = scores_22.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores_22['Identical Score Percentage'] = scores_22.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(scores_22.head(20))"
   ],
   "id": "c2606d1c7c982f93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "1   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "2    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "3   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "4   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "5   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "6    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "7   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "8   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "9   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "10  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "11  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "12  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "13  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "14   1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "15  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "16  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "17  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "18   1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "19  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next i try and figure out a good configuration for temperature and/or top_p. Since gpt-4-turbo is 20x more expensive than gpt-3.5-turbo i will only test the gpt-3.5-turbo model. ",
   "id": "d04efed1e7938bb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "R3-1-1 to R3-1-5 \n",
    "\n",
    "Parameters:\n",
    "- 1.prompt = f\"Consider the sentiment of this article towards {bank_name}. If {bank_name} is not mentioned, assign a neutral score of 0.5. Otherwise, score the sentiment from 0 to 1, where 0 is very negative and 1 is very positive. Provide the sentiment score followed by a brief explanation in less than 20 words, separating them with a period. Example: '[insert sentiment score]. [insert explanation]'.\"\n",
    "    - scaling from 0 to 1 continuous in 0.1 steps\n",
    "- temperature = 0, top_p = 1 \n",
    "- model = gpt-3.5-turbo-0125\n",
    "- non-anonymized data"
   ],
   "id": "20a970c4fffadc0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:54:50.397732Z",
     "start_time": "2024-05-13T13:54:50.131778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "    \n",
    "    \n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names_31 = ['Output_Testdata/CS/sample_data_20_20240512_202901_R3-1-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_203037_R3-1-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_203114_R3-1-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_203232_R3-1-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_203319_R3-1-5.csv']\n",
    "file_names_32 = ['Output_Testdata/CS/sample_data_20_20240512_203456_R3-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_203622_R3-2-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_203718_R3-2-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_203814_R3-2-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_203925_R3-2-5.csv']\n",
    "file_names_33 = ['Output_Testdata/CS/sample_data_20_20240512_204052_R3-3-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_204251_R3-3-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_204612_R3-3-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_204754_R3-3-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_204912_R3-3-5.csv']\n",
    "file_names_34 = ['Output_Testdata/CS/sample_data_20_20240512_205205_R3-4-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_205320_R3-4-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_205414_R3-4-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_205519_R3-4-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_205614_R3-4-5.csv']\n",
    "file_names_35 = ['Output_Testdata/CS/sample_data_20_20240512_205856_R3-5-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_210041_R3-5-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_210120_R3-5-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_210215_R3-5-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_210312_R3-5-5.csv']\n",
    "file_names_36 = ['Output_Testdata/CS/sample_data_20_20240512_210509_R3-6-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_210624_R3-6-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_210715_R3-6-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_210820_R3-6-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_210916_R3-6-5.csv']\n",
    "file_names_37 = ['Output_Testdata/CS/sample_data_20_20240512_211111_R3-7-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_211236_R3-7-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_211344_R3-7-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_211504_R3-7-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_211630_R3-7-5.csv']\n",
    "file_names_38 = ['Output_Testdata/CS/sample_data_20_20240512_211804_R3-8-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_212418_R3-8-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_212516_R3-8-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_212615_R3-8-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_212759_R3-8-5.csv']\n",
    "\n",
    "\n",
    "dataframes_31 = [pd.read_csv(file_name) for file_name in file_names_31]\n",
    "dataframes_32 = [pd.read_csv(file_name) for file_name in file_names_32]\n",
    "dataframes_33 = [pd.read_csv(file_name) for file_name in file_names_33]\n",
    "dataframes_34 = [pd.read_csv(file_name) for file_name in file_names_34]\n",
    "dataframes_35 = [pd.read_csv(file_name) for file_name in file_names_35]\n",
    "dataframes_36 = [pd.read_csv(file_name) for file_name in file_names_36]\n",
    "dataframes_37 = [pd.read_csv(file_name) for file_name in file_names_37]\n",
    "dataframes_38 = [pd.read_csv(file_name) for file_name in file_names_38]\n",
    "\n",
    "for df_31 in dataframes_31:\n",
    "    df_31['Extracted_Score'] = df_31['Sentiment'].apply(extract_score)\n",
    "for df_32 in dataframes_32:\n",
    "    df_32['Extracted_Score'] = df_32['Sentiment'].apply(extract_score)\n",
    "for df_33 in dataframes_33:\n",
    "    df_33['Extracted_Score'] = df_33['Sentiment'].apply(extract_score)\n",
    "for df_34 in dataframes_34:\n",
    "    df_34['Extracted_Score'] = df_34['Sentiment'].apply(extract_score)\n",
    "for df_35 in dataframes_35:\n",
    "    df_35['Extracted_Score'] = df_35['Sentiment'].apply(extract_score)\n",
    "for df_36 in dataframes_36:\n",
    "    df_36['Extracted_Score'] = df_36['Sentiment'].apply(extract_score)\n",
    "for df_37 in dataframes_37:\n",
    "    df_37['Extracted_Score'] = df_37['Sentiment'].apply(extract_score)\n",
    "for df_38 in dataframes_38:\n",
    "    df_38['Extracted_Score'] = df_38['Sentiment'].apply(extract_score)\n",
    "\n",
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores_31 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_31['Extracted_Score'] for i, df_31 in enumerate(dataframes_31)\n",
    "})\n",
    "scores_32 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_32['Extracted_Score'] for i, df_32 in enumerate(dataframes_32)\n",
    "})\n",
    "scores_33 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_33['Extracted_Score'] for i, df_33 in enumerate(dataframes_33)\n",
    "})\n",
    "scores_34 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_34['Extracted_Score'] for i, df_34 in enumerate(dataframes_34)\n",
    "})\n",
    "scores_35 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_35['Extracted_Score'] for i, df_35 in enumerate(dataframes_35)\n",
    "})\n",
    "scores_36 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_36['Extracted_Score'] for i, df_36 in enumerate(dataframes_36)\n",
    "})\n",
    "scores_37 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_37['Extracted_Score'] for i, df_37 in enumerate(dataframes_37)\n",
    "})\n",
    "scores_38 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_38['Extracted_Score'] for i, df_38 in enumerate(dataframes_38)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores_31['Mean'] = scores_31.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "scores_32['Mean'] = scores_32.mean(axis=1)\n",
    "scores_33['Mean'] = scores_33.mean(axis=1)\n",
    "scores_34['Mean'] = scores_34.mean(axis=1)\n",
    "scores_35['Mean'] = scores_35.mean(axis=1)\n",
    "scores_36['Mean'] = scores_36.mean(axis=1)\n",
    "scores_37['Mean'] = scores_37.mean(axis=1)\n",
    "scores_38['Mean'] = scores_38.mean(axis=1)\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores_31['Identical Score Percentage'] = scores_31.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_32['Identical Score Percentage'] = scores_32.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_33['Identical Score Percentage'] = scores_33.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_34['Identical Score Percentage'] = scores_34.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_35['Identical Score Percentage'] = scores_35.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_36['Identical Score Percentage'] = scores_36.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_37['Identical Score Percentage'] = scores_37.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_38['Identical Score Percentage'] = scores_38.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(\"top_p = 1, temperature = 0\")\n",
    "print(scores_31.head(20)) \n",
    "print(\"top_p = 1, temperature = 0.3\")\n",
    "print(scores_32.head(20)) \n",
    "print(\"top_p = 1, temperature = 0.6\")\n",
    "print(scores_33.head(20)) \n",
    "print(\"top_p = 1, temperature = 0.9\")\n",
    "print(scores_34.head(20)) \n",
    "print(\"top_p = 0, temperature = 1\")\n",
    "print(scores_35.head(20)) \n",
    "print(\"top_p = 0.3, temperature = 0\")\n",
    "print(scores_36.head(20)) \n",
    "print(\"top_p = 0.6, temperature = 0\")\n",
    "print(scores_37.head(20)) \n",
    "print(\"top_p = 0.9, temperature = 0\")\n",
    "print(scores_38.head(20)) \n"
   ],
   "id": "51cb4e2b58d07d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_p = 1, temperature = 0\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "3    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.5   0.5   0.5   0.5   0.5  0.50                       100.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "8    0.6   0.6   0.6   0.4   0.6  0.56                        80.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "11   0.3   0.3   0.2   0.3   0.3  0.28                        80.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "15   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "16   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "17   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "top_p = 1, temperature = 0.3\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.6   0.7   0.7   0.6  0.66                        60.0\n",
      "3    0.6   0.6   0.7   0.7   0.7  0.66                        60.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.5   0.5   0.5   0.5   0.5  0.50                       100.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.2   0.2   0.3   0.3   0.2  0.24                        60.0\n",
      "8    0.6   0.4   0.3   0.3   0.5  0.42                        40.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.7   0.8   0.7   0.8   0.8  0.76                        60.0\n",
      "11   0.3   0.2   0.4   0.3   0.3  0.30                        60.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7   0.8   0.7   0.7   0.8  0.74                        60.0\n",
      "15   0.4   0.7   0.3   0.7   0.3  0.48                        40.0\n",
      "16   0.6   0.7   0.8   0.7   0.7  0.70                        60.0\n",
      "17   0.2   0.3   0.3   0.3   0.3  0.28                        80.0\n",
      "18   0.7   0.8   0.7   0.7   0.6  0.70                        60.0\n",
      "19   0.4   0.3   0.6   0.3   0.3  0.38                        60.0\n",
      "top_p = 1, temperature = 0.6\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7  0.80   0.7   0.8   0.7  0.74                        60.0\n",
      "1    0.2  0.20   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7  0.60   0.6   0.6   0.7  0.64                        60.0\n",
      "3    0.7  0.70   0.7   0.7   0.7  0.70                       100.0\n",
      "4    0.2  0.20   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.7  0.50   0.5   0.5   0.5  0.54                        80.0\n",
      "6    0.8  0.80   0.7   0.8   0.8  0.78                        80.0\n",
      "7    0.3  0.30   0.2   0.3   0.2  0.26                        60.0\n",
      "8    0.4  0.60   0.6   0.6   0.4  0.52                        60.0\n",
      "9    0.2  0.20   0.2   0.2   0.3  0.22                        80.0\n",
      "10   0.8  0.80   0.7   0.7   0.7  0.74                        60.0\n",
      "11   0.2  0.20   0.2   0.3   0.3  0.24                        60.0\n",
      "12   0.3  0.30   0.2   0.3   0.4  0.30                        60.0\n",
      "13   0.2  0.20   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7  0.70   0.8   0.7   0.7  0.72                        80.0\n",
      "15   0.7  0.30   0.6   0.4   0.3  0.46                        40.0\n",
      "16   0.7  0.80   0.7   0.6   0.6  0.68                        40.0\n",
      "17   0.4  0.30   0.2   0.2   0.3  0.28                        40.0\n",
      "18   0.8  0.80   0.7   0.8   0.7  0.76                        60.0\n",
      "19   0.6  0.35   0.2   0.4   0.2  0.35                        40.0\n",
      "top_p = 1, temperature = 0.9\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0   0.70   0.6   0.6  0.80   0.7  0.68                        40.0\n",
      "1   0.30   0.2   0.2  0.20   0.2  0.22                        80.0\n",
      "2   0.60   0.6   0.7  0.60   0.7  0.64                        60.0\n",
      "3   0.70   0.7   0.7  0.70   0.7  0.70                       100.0\n",
      "4   0.20   0.3   0.2  0.20   0.2  0.22                        80.0\n",
      "5   0.50   0.5   0.5  0.60   0.5  0.52                        80.0\n",
      "6   0.70   0.7   0.8  0.80   0.7  0.74                        60.0\n",
      "7   0.20   0.2   0.2  0.20   0.3  0.22                        80.0\n",
      "8   0.70   0.4   0.5  0.70   0.2  0.50                        40.0\n",
      "9   0.30   0.2   0.2  0.30   0.3  0.26                        60.0\n",
      "10  0.60   0.8   0.7  0.70   0.6  0.68                        40.0\n",
      "11  0.30   0.3   0.3  0.40   0.3  0.32                        80.0\n",
      "12  0.30   0.4   0.4  0.20   0.2  0.30                        40.0\n",
      "13  0.20   0.3   0.2  0.20   0.2  0.22                        80.0\n",
      "14  0.85   0.6   0.8  0.60   0.7  0.71                        40.0\n",
      "15  0.30   0.4   0.8  0.40   0.3  0.44                        40.0\n",
      "16  0.75   0.8   0.7  0.70   0.7  0.73                        60.0\n",
      "17  0.30   0.2   0.2  0.20   0.4  0.26                        60.0\n",
      "18  0.70   0.7   0.8  0.75   0.6  0.71                        40.0\n",
      "19  0.60   0.6   0.3  0.40   0.2  0.42                        40.0\n",
      "top_p = 0, temperature = 1\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.6   0.7   0.7   0.7  0.68                        80.0\n",
      "3    0.7   0.7   0.7   0.6   0.7  0.68                        80.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.5   0.5   0.5   0.5   0.5  0.50                       100.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.3   0.3   0.3   0.2   0.3  0.28                        80.0\n",
      "8    0.6   0.4   0.4   0.6   0.6  0.52                        60.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "11   0.3   0.2   0.3   0.3   0.3  0.28                        80.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "15   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "16   0.7   0.8   0.8   0.7   0.8  0.76                        60.0\n",
      "17   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "top_p = 0.3, temperature = 0\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "3    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.5   0.5   0.5   0.5   0.5  0.50                       100.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.2   0.3   0.3   0.3   0.3  0.28                        80.0\n",
      "8    0.6   0.4   0.6   0.6   0.4  0.52                        60.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.7   0.7   0.7   0.8   0.7  0.72                        80.0\n",
      "11   0.3   0.2   0.2   0.2   0.2  0.22                        80.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "15   0.4   0.4   0.3   0.3   0.3  0.34                        60.0\n",
      "16   0.7   0.8   0.8   0.7   0.8  0.76                        60.0\n",
      "17   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.3   0.2   0.3   0.4   0.3  0.30                        60.0\n",
      "top_p = 0.6, temperature = 0\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.8   0.8   0.7  0.74                        60.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "3    0.6   0.6   0.7   0.7   0.7  0.66                        60.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.5   0.5   0.5   0.7   0.5  0.54                        80.0\n",
      "6    0.7   0.8   0.8   0.8   0.8  0.78                        80.0\n",
      "7    0.3   0.3   0.3   0.3   0.2  0.28                        80.0\n",
      "8    0.7   0.6   0.6   0.3   0.7  0.58                        40.0\n",
      "9    0.2   0.3   0.3   0.3   0.3  0.28                        80.0\n",
      "10   0.7   0.8   0.8   0.8   0.7  0.76                        60.0\n",
      "11   0.3   0.3   0.3   0.2   0.2  0.26                        60.0\n",
      "12   0.2   0.2   0.3   0.3   0.3  0.26                        60.0\n",
      "13   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7   0.8   0.7   0.8   0.8  0.76                        60.0\n",
      "15   0.6   0.3   0.3   0.4   0.3  0.38                        60.0\n",
      "16   0.7   0.7   0.7   0.8   0.8  0.74                        60.0\n",
      "17   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.3   0.3   0.4   0.2   0.4  0.32                        40.0\n",
      "top_p = 0.9, temperature = 0\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0   0.80   0.7   0.8   0.8   0.7  0.76                        60.0\n",
      "1   0.20   0.3   0.3   0.2   0.2  0.24                        60.0\n",
      "2   0.80   0.7   0.7   0.8   0.8  0.76                        60.0\n",
      "3   0.70   0.6   0.6   0.7   0.7  0.66                        60.0\n",
      "4   0.20   0.2   0.2   0.2   0.3  0.22                        80.0\n",
      "5   0.60   0.8   0.5   0.7   0.6  0.64                        40.0\n",
      "6   0.70   0.7   0.8   0.7   0.7  0.72                        80.0\n",
      "7   0.30   0.3   0.3   0.3   0.2  0.28                        80.0\n",
      "8   0.60   0.4   0.6   0.6   0.3  0.50                        60.0\n",
      "9   0.20   0.3   0.2   0.3   0.3  0.26                        60.0\n",
      "10  0.80   0.6   0.8   0.8   0.7  0.74                        60.0\n",
      "11  0.30   0.2   0.2   0.3   0.3  0.26                        60.0\n",
      "12  0.30   0.3   0.2   0.3   0.4  0.30                        60.0\n",
      "13  0.10   0.2   0.2   0.3   0.3  0.22                        40.0\n",
      "14  0.75   0.7   0.6   0.8   0.7  0.71                        40.0\n",
      "15  0.30   0.3   0.3   0.6   0.4  0.38                        60.0\n",
      "16  0.60   0.8   0.6   0.8   0.8  0.72                        60.0\n",
      "17  0.30   0.4   0.2   0.2   0.3  0.28                        40.0\n",
      "18  0.70   0.7   0.6   0.6   0.7  0.66                        60.0\n",
      "19  0.20   0.2   0.3   0.2   0.6  0.30                        60.0\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So as expected, the higher the values for top_p and temperature the less deterministic the model becomes. Anything above 0.3 doesnt make sense for this task. Even 0.3 seems too high. The values dont change much so the sentiment is still scored the same way. So the quality is about the same, but the consistency is worse. \n",
    "I think sticking with top_p = 0 and temperature = 0 is the best choice for this task."
   ],
   "id": "87b7327da84c1079"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T20:24:54.566764Z",
     "start_time": "2024-05-12T20:24:54.538599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(scores_11.head(20))\n",
    "print(scores_31.head(20))"
   ],
   "id": "f3dd9ed7b9ff35da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "3    0.7   0.7   0.6   0.6   0.7  0.66                        60.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.5   0.5   0.5   0.5   0.5  0.50                       100.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "8    0.6   0.6   0.6   0.6   0.6  0.60                       100.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "11   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "15   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "16   0.7   0.7   0.8   0.7   0.8  0.74                        60.0\n",
      "17   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "3    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.5   0.5   0.5   0.5   0.5  0.50                       100.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "8    0.6   0.6   0.6   0.4   0.6  0.56                        80.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "11   0.3   0.3   0.2   0.3   0.3  0.28                        80.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "14   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "15   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "16   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "17   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf8b88cf0f71cd03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "comparing R1-1-1 (topp = 0,temp = 0) with R3-1-1 (topp=1, temp=0) \n",
    "\n",
    "Setting topp to 1 seems to improve consistency slightly, but they are pretty much the same.\n",
    "\n",
    "\n"
   ],
   "id": "73679bfb97eb6f40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c036d54d6f4629c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So the best parameters so far in terms of consistency are: \n",
    "\n",
    "prompt: \n",
    "- 1st and 3rd prompt\n",
    "\n",
    "topp and temperature: \n",
    "- either both 0 or temperature = 0 and topp = 1 (because its recommended to only adjust one of the parameters at a time)\n",
    "- model: gpt-4-turbo-2024-04-09\n",
    "\n",
    "And in terms of quality the best prompt is the 3rd prompt combined with the gpt-4-turbo-2024-04-09 model. Although the 1st prompt with gpt-4-turbo-2024-04-09 is only slightly worse in terms of quality but gives more detailed scores. "
   ],
   "id": "e72105c36a17b307"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next ill test the following: \n",
    "\n",
    "Test R4-1-1 to R4-1-5:\n",
    "Prompt3, topp = 1, temperature = 0, gpt 3.5\n",
    "\n",
    "Test R4-2-1 to R4-2-5:\n",
    "Prompt3, topp = 1, temperature = 0, gpt 4\n",
    "\n",
    "Test R4-3-1 to R4-3-5:\n",
    "Prompt1, topp = 1, temperature = 0, gpt 4\n"
   ],
   "id": "664fcd8e0fdd4120"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:32:12.265692Z",
     "start_time": "2024-05-13T12:32:12.143363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "    \n",
    "    \n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names_41 = ['Output_Testdata/CS/sample_data_20_20240512_231921_R4-1-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_232018_R4-1-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_232158_R4-1-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_232315_R4-1-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_232419_R4-1-5.csv']\n",
    "file_names_42 = ['Output_Testdata/CS/sample_data_20_20240512_232900_R4-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_233045_R4-2-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_233228_R4-2-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_233324_R4-2-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_233420_R4-2-5.csv']\n",
    "file_names_43 = ['Output_Testdata/CS/sample_data_20_20240512_233702_R4-3-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_233816_R4-3-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_233920_R4-3-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_234049_R4-3-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_234257_R4-3-5.csv']\n",
    "\n",
    "\n",
    "dataframes_41 = [pd.read_csv(file_name) for file_name in file_names_41]\n",
    "dataframes_42 = [pd.read_csv(file_name) for file_name in file_names_42]\n",
    "dataframes_43 = [pd.read_csv(file_name) for file_name in file_names_43]\n",
    "\n",
    "\n",
    "for df_41 in dataframes_41:\n",
    "    df_41['Extracted_Score'] = df_41['Sentiment'].apply(extract_score)\n",
    "for df_42 in dataframes_42:\n",
    "    df_42['Extracted_Score'] = df_42['Sentiment'].apply(extract_score)\n",
    "for df_43 in dataframes_43:\n",
    "    df_43['Extracted_Score'] = df_43['Sentiment'].apply(extract_score)\n",
    "\n",
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores_41 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_41['Extracted_Score'] for i, df_41 in enumerate(dataframes_41)\n",
    "})\n",
    "scores_42 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_42['Extracted_Score'] for i, df_42 in enumerate(dataframes_42)\n",
    "})\n",
    "scores_43 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_43['Extracted_Score'] for i, df_43 in enumerate(dataframes_43)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores_41['Mean'] = scores_41.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "scores_42['Mean'] = scores_42.mean(axis=1)\n",
    "scores_43['Mean'] = scores_43.mean(axis=1)\n",
    "\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores_41['Identical Score Percentage'] = scores_41.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_42['Identical Score Percentage'] = scores_42.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "scores_43['Identical Score Percentage'] = scores_43.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(\"3rd prompt, top_p = 1, temperature = 0\")\n",
    "print(scores_41.head(20)) \n",
    "print(\"3rd prompt, top_p = 1, temperature = 0\")\n",
    "print(scores_42.head(20)) \n",
    "print(\"1st prompt, top_p = 1, temperature = 0\")\n",
    "print(scores_43.head(20)) \n"
   ],
   "id": "cd9f5e7291316945",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd prompt, top_p = 1, temperature = 0\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "1   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "2   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "3   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "4   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "5   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "6    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "7   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "8   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "9   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "10   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "11  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "12  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "13  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "14  -1.0  -1.0  -1.0   0.0  -1.0  -0.8                        80.0\n",
      "15  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "16   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "17  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "18   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "19  -1.0  -1.0   0.0  -1.0   0.0  -0.6                        60.0\n",
      "3rd prompt, top_p = 1, temperature = 0\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "1   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "2    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "3   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "4   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "5   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "6    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "7   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "8   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "9   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "10  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "11  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "12  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "13  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "14   1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "15  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "16  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "17  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "18   1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "19  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "1st prompt, top_p = 1, temperature = 0\n",
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "1    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "2    0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "3    0.4   0.4   0.3   0.4   0.4  0.38                        80.0\n",
      "4    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "5    0.1   0.1   0.1   0.1   0.1  0.10                       100.0\n",
      "6    0.8   0.8   0.8   0.8   0.8  0.80                       100.0\n",
      "7    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "8    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "9    0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "10   0.4   0.4   0.4   0.4   0.4  0.40                       100.0\n",
      "11   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "12   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "13   0.1   0.1   0.1   0.1   0.1  0.10                       100.0\n",
      "14   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "15   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "16   0.3   0.3   0.3   0.3   0.3  0.30                       100.0\n",
      "17   0.2   0.2   0.2   0.2   0.2  0.20                       100.0\n",
      "18   0.7   0.7   0.7   0.7   0.7  0.70                       100.0\n",
      "19   0.2   0.2   0.2   0.1   0.2  0.18                        80.0\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9786e83b5c763934"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "setting topp to and temperature to 0 or setting both to 0 seems to result in almost the same consistency. I think i will follow the recommendation of only adjusting one parameter at a time and go with temperature = 0 and topp = 1. This might also have a positive effect on the explanation part of the output. So the top model would be: 3rd prompt, topp = 1, temperature = 0, gpt 4 turbo. Now i want to find out if anonymizing the data has an effect on the scores. \n",
   "id": "d08e0a5ca507f0ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c2fd390e331b7608"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Test R5-1-1 to R5-1-5:\n",
    "Prompt3, top = 1, temperature = 0, gpt 3.5, anonymized data\n",
    "Test R5-2-1 to R5-2-5:\n",
    "Prompt3, top = 1, temperature = 0, gpt 4, anonymized data\n"
   ],
   "id": "fdc7ac65037c5230"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:34:23.072165Z",
     "start_time": "2024-05-12T22:34:23.020513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names = ['Output_Testdata/CS/anonymized_data_20_20240513_001258_R5-1-1.csv',\n",
    "              'Output_Testdata/CS/anonymized_data_20_20240513_001657_R5-1-2.csv',\n",
    "              'Output_Testdata/CS/anonymized_data_20_20240513_001820_R5-1-3.csv',\n",
    "              'Output_Testdata/CS/anonymized_data_20_20240513_001920_R5-1-4.csv',\n",
    "              'Output_Testdata/CS/anonymized_data_20_20240513_002024_R5-1-5.csv']\n",
    "dataframes = [pd.read_csv(file_name) for file_name in file_names]\n",
    "\n",
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df in dataframes:\n",
    "    df['Extracted_Score'] = df['Sentiment'].apply(extract_score)\n",
    "    \n",
    "\n",
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores = pd.DataFrame({\n",
    "    f'Run{i+1}': df['Extracted_Score'] for i, df in enumerate(dataframes)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores['Mean'] = scores.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores['Identical Score Percentage'] = scores.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(scores.head(20))"
   ],
   "id": "58194a634129aad0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "1   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "2   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "3   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "4   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "5   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "6    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "7   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "8   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "9   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "10   0.0   0.0   0.0  -1.0   0.0  -0.2                        80.0\n",
      "11  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "12  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "13  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "14   0.0   0.0  -1.0   0.0   0.0  -0.2                        80.0\n",
      "15  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "16   0.0   0.0   0.0   0.0  -1.0  -0.2                        80.0\n",
      "17  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "18   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "19  -1.0  -1.0  -1.0  -1.0   0.0  -0.8                        80.0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65e80bae7ff8d68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1a7442684b31830b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:38:32.010165Z",
     "start_time": "2024-05-13T13:38:31.963687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names_52 = ['Output_Testdata/CS/anonymized_data_20_20240513_002442_R5-2-1.csv',\n",
    "              'Output_Testdata/CS/anonymized_data_20_20240513_002700_R5-2-2.csv',\n",
    "              'Output_Testdata/CS/anonymized_data_20_20240513_002829_R5-2-3.csv',\n",
    "              'Output_Testdata/CS/anonymized_data_20_20240513_002925_R5-2-4.csv',\n",
    "              'Output_Testdata/CS/anonymized_data_20_20240513_003019_R5-2-5.csv']\n",
    "dataframes_52 = [pd.read_csv(file_name) for file_name in file_names_52]\n",
    "\n",
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df_52 in dataframes_52:\n",
    "    df_52['Extracted_Score'] = df_52['Sentiment'].apply(extract_score)\n",
    "    \n",
    "\n",
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores_52 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_52['Extracted_Score'] for i, df_52 in enumerate(dataframes_52)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores_52['Mean'] = scores_52.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores_52['Identical Score Percentage'] = scores_52.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(scores_52.head(20))"
   ],
   "id": "55f4b3ed9e3525f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "1   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "2    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "3   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "4   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "5   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "6    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "7   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "8   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "9   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "10  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "11  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "12  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "13  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "14   1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "15  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "16  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "17  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "18   1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "19  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Choosing the right model: ",
   "id": "2d5b88638c6e9922"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Consistency\n",
    "### R1 Tests: \n",
    "parameters:\n",
    "    prompt 1 vs 2 vs 3,\n",
    "        with gpt-3.5-turbo, top_p = 0, temp = 0, non-anonymized data\n",
    "\n",
    "R1 showed that prompt 1 and prompt 3 performed best. Prompt 1 was inconsistent with 2 articles and prompt 3 with 3 articles. \n",
    "\n",
    "### R2 Tests:\n",
    "parameters:\n",
    "    prompt 1 vs 3,\n",
    "        with gpt-4-turbo, top_p = 0, temp = 0, non-anonymized data\n",
    "\n",
    "R2 showed that prompt 1 was inconsistent in only 1 article and prompt 3 was fully consistent. \n",
    "\n",
    "### R3 Tests: \n",
    "parameters:\n",
    "    top_p=1, temp = 0 vs\n",
    "    top_p=1, temp = 0.3 vs\n",
    "    top_p=1, temp = 0.6 vs\n",
    "    top_p=1, temp = 0.9 vs\n",
    "    top_p=0, temp = 1 vs\n",
    "    top_p=0.3, temp = 0 vs\n",
    "    top_p=0.6, temp = 0 vs\n",
    "    top_p=0.9, temp = 0 vs\n",
    "        with prompt 1, gpt-3.5-turbo, non-anonymized data, \n",
    "\n",
    "R3 showed that top_p = 1 and temp = 0 was the most consistent with 2 articles being inconsistent (so same as with top_p = 0 and temperature = 0)\n",
    "\n",
    "### R4 Tests:\n",
    "parameters:\n",
    "    prompt 3, top_p = 1, temp = 0, gpt-3.5-turbo, non-anonymized data\n",
    "    -> inconsistent with 2 articles (with top_p = 0 and temp = 0 it was inconsistent with 3 articles)\n",
    "    prompt 3, top_p = 1, temp = 0, gpt-4-turbo, non-anonymized data\n",
    "    -> fully consistent (same as with top_p = 0 and temp = 0)\n",
    "    prompt 1, top_p = 1, temp = 0, gpt-4-turbo, non-anonymized data\n",
    "    -> inconsistent with 2 articles (with top_p = 0 and temp = 0 it was inconsistent with 1 article)\n",
    "\n",
    "### R5 Tests:\n",
    "parameters:\n",
    "    prompt 3, top_p = 1, temp = 0, gpt-3.5-turbo, anonymized data\n",
    "    -> inconsistent with 4 articles (with non-anonymized data it was inconsistent with 2 articles)\n",
    "    prompt 3, top_p = 1, temp = 0, gpt-4-turbo, anonymized data\n",
    "    -> fully consistent (same as with non-anonymized data)\n",
    "\n",
    "R5 showed that, with the gpt-3.5 model, anonymizing data had a negative effect on consistency. With the gpt-4 model, anonymizing data had no effect on consistency.\n",
    "\n",
    "### Conclusion:\n",
    "Top models are: \n",
    "- R4-2: prompt 3, top_p = 1, temp = 0, gpt-4-turbo, non-anonymized data\n",
    "    -> fully consistent\n",
    "- R5-2: prompt 3, top_p = 1, temp = 0, gpt-4-turbo, anonymized data\n",
    "    -> fully consistent\n",
    "- R2-2: prompt 3, top_p = 0, temp = 0, gpt-4-turbo, non-anonymized data\n",
    "    -> fully consistent\n",
    "- R2-1: prompt 1, top_p = 0, temp = 0, gpt-4-turbo, non-anonymized data\n",
    "    -> inconsistent with 1 article\n",
    "- R4-3: prompt 1, top_p = 1, temp = 0, gpt-4-turbo, non-anonymized data\n",
    "    -> inconsistent with 2 articles\n",
    "- R1-1: prompt 1, top_p = 0, temp = 0, gpt-3.5-turbo, non-anonymized data\n",
    "    -> inconsistent with 2 articles\n",
    "- R3-1: prompt 1, top_p = 0, temp = 1, gpt-3.5-turbo, non-anonymized data\n",
    "    -> inconsistent with 2 articles\n",
    "- R4-1: prompt 3, top_p = 1, temp = 0, gpt-3.5-turbo, non-anonymized data\n",
    "    -> inconsistent with 2 articles\n",
    "\n"
   ],
   "id": "e60f3aafd33554e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Quality of sentiment evaluation\n",
    "\n",
    "### Personal evaluation of articles:\n",
    "Article 0: Probably neutral to positiv. Some positive sentiment (returns up), CS generally in a difficult position. \n",
    "\n",
    "Article 1: Very negative (Archegos stuff)\n",
    "\n",
    "Article 2: Neutral – CS providing insights \n",
    "\n",
    "Article 3: Negative – spying scandal\n",
    "\n",
    "Article 4: Eher negative (AT1 bonds after takeover)\n",
    "\n",
    "Article 5: Eher negative (article is mainly about deutsche bank not being the next credit Suisse)\n",
    "\n",
    "Article 6: (Neutral,) slightly positive (CS helping Climate bonds initiative)\n",
    "\n",
    "Article 7: Negative – lawsuits blabla\n",
    "\n",
    "Article 8: Neutral to slightly negative. (princeling hiring practice in banking sector)\n",
    "\n",
    "Article 9: Negative (credit Suisse takeover, restructuring blabla…) \n",
    "\n",
    "Article 10: More or less neutral, negativ towards previous management, optimistic about new management. Could be anything honestly. \n",
    "\n",
    "Article 11: Eher negativ\n",
    "\n",
    "Article 12: Eher negativ\n",
    "\n",
    "Article 13: Negativ.\n",
    "\n",
    "Article 14: Neutral to slightly positiv. \n",
    "\n",
    "Article 15: Negativ \n",
    "\n",
    "Article 16: Slightly negativ to neutral\n",
    "\n",
    "Article 17: Negativ\n",
    "\n",
    "Article 18: Optimistic, probably neutral to slightly positiv\n",
    "\n",
    "Article 19: Handelt von der PUK – neutral to slightly negativ\n",
    "\n",
    "\n",
    "### Evaluation Procedure: \n",
    "\n",
    "If the gpt evaluation is consistent with my evaluation, ill give a point. If it is off completely ill give 0 points. Some articles are hard to evaluate because they have both negative and positive aspects. "
   ],
   "id": "db4348ff589b0aaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### R4-2, R5-2, R2-2 have the same scores\n",
    "\n",
    "0. good\n",
    "1. good\n",
    "2. good\n",
    "3. good\n",
    "4. good\n",
    "5. good\n",
    "6. good\n",
    "7. good\n",
    "8. good\n",
    "9. good\n",
    "10. good\n",
    "11. good\n",
    "12. good\n",
    "13. good\n",
    "14. good\n",
    "15. good\n",
    "16. good\n",
    "17. good\n",
    "18. good\n",
    "19. good\n",
    "\n",
    "--> 20 points"
   ],
   "id": "602985b442bf9c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:44:08.327096Z",
     "start_time": "2024-05-13T13:44:08.309474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_df = pd.concat([scores_42[\"Mean\"], scores_52[\"Mean\"], scores_22[\"Mean\"]], axis=1)\n",
    "\n",
    "# Optionally, setting new column names\n",
    "new_df.columns = ['R42', 'R52', 'R22']\n",
    "\n",
    "new_df"
   ],
   "id": "b82f27f9f42d4cb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    R42  R52  R22\n",
       "0   1.0  1.0  1.0\n",
       "1  -1.0 -1.0 -1.0\n",
       "2   0.0  0.0  0.0\n",
       "3  -1.0 -1.0 -1.0\n",
       "4  -1.0 -1.0 -1.0\n",
       "5  -1.0 -1.0 -1.0\n",
       "6   1.0  1.0  1.0\n",
       "7  -1.0 -1.0 -1.0\n",
       "8  -1.0 -1.0 -1.0\n",
       "9  -1.0 -1.0 -1.0\n",
       "10 -1.0 -1.0 -1.0\n",
       "11 -1.0 -1.0 -1.0\n",
       "12 -1.0 -1.0 -1.0\n",
       "13 -1.0 -1.0 -1.0\n",
       "14  1.0  1.0  1.0\n",
       "15 -1.0 -1.0 -1.0\n",
       "16 -1.0 -1.0 -1.0\n",
       "17 -1.0 -1.0 -1.0\n",
       "18  1.0  1.0  1.0\n",
       "19 -1.0 -1.0 -1.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R42</th>\n",
       "      <th>R52</th>\n",
       "      <th>R22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### R2-1 and R4-3 have the same scores\n",
    "\n",
    "0. good\n",
    "1. good\n",
    "2. bad\n",
    "3. bad\n",
    "4. good\n",
    "5. good\n",
    "6. good\n",
    "7. good\n",
    "8. good\n",
    "9. good\n",
    "10. good\n",
    "11. good\n",
    "12. good\n",
    "13. good\n",
    "14. good\n",
    "15. good\n",
    "16. good\n",
    "17. good\n",
    "18. good\n",
    "19. good\n",
    "\n",
    "-> 18 points"
   ],
   "id": "26f99c805a89bd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:53:28.989050Z",
     "start_time": "2024-05-13T13:53:28.973339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_df = pd.concat([scores_21[\"Mean\"], scores_43[\"Mean\"]], axis=1)\n",
    "\n",
    "# Optionally, setting new column names\n",
    "new_df.columns = ['R21', 'R43']\n",
    "\n",
    "new_df"
   ],
   "id": "591f4be53d194e0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     R21   R43\n",
       "0   0.70  0.70\n",
       "1   0.20  0.20\n",
       "2   0.70  0.70\n",
       "3   0.40  0.38\n",
       "4   0.20  0.20\n",
       "5   0.14  0.10\n",
       "6   0.80  0.80\n",
       "7   0.20  0.20\n",
       "8   0.20  0.20\n",
       "9   0.20  0.20\n",
       "10  0.40  0.40\n",
       "11  0.20  0.20\n",
       "12  0.30  0.30\n",
       "13  0.10  0.10\n",
       "14  0.70  0.70\n",
       "15  0.20  0.20\n",
       "16  0.30  0.30\n",
       "17  0.20  0.20\n",
       "18  0.70  0.70\n",
       "19  0.20  0.18"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R21</th>\n",
       "      <th>R43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "dataframes = []\n",
    "for i, file_name in enumerate(file_names):\n",
    "    df = pd.read_csv(file_name)\n",
    "    # Assume 'Sentiment' is the column to be extracted\n",
    "    df = df[['Sentiment']].rename(columns={'Sentiment': f'Run{i+1}'})\n",
    "    dataframes.append(df)"
   ],
   "id": "11adc7ca69d52f50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### R1-1 and R3-1 have the same scores\n",
    "\n",
    "0. good\n",
    "1. good\n",
    "2. bad\n",
    "3. bad\n",
    "4. good\n",
    "5. bad\n",
    "6. good\n",
    "7. good\n",
    "8. ok\n",
    "9. good\n",
    "10. good\n",
    "11. good\n",
    "12. good\n",
    "13. good\n",
    "14. good\n",
    "15. good\n",
    "16. bad\n",
    "17. good\n",
    "18. good\n",
    "19. good\n",
    "\n",
    "-> 16 points\n"
   ],
   "id": "2f9a24cd9324475"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:54:55.669318Z",
     "start_time": "2024-05-13T13:54:55.651802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_df = pd.concat([scores_11[\"Mean\"], scores_31[\"Mean\"]], axis=1)\n",
    "\n",
    "# Optionally, setting new column names\n",
    "new_df.columns = ['R11', 'R31']\n",
    "\n",
    "new_df"
   ],
   "id": "422bd0d12bdf7f7a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     R11   R31\n",
       "0   0.70  0.70\n",
       "1   0.20  0.20\n",
       "2   0.70  0.70\n",
       "3   0.66  0.70\n",
       "4   0.20  0.20\n",
       "5   0.50  0.50\n",
       "6   0.80  0.80\n",
       "7   0.30  0.30\n",
       "8   0.60  0.56\n",
       "9   0.20  0.20\n",
       "10  0.70  0.70\n",
       "11  0.30  0.28\n",
       "12  0.30  0.30\n",
       "13  0.20  0.20\n",
       "14  0.70  0.70\n",
       "15  0.30  0.30\n",
       "16  0.74  0.70\n",
       "17  0.30  0.30\n",
       "18  0.70  0.70\n",
       "19  0.30  0.30"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R11</th>\n",
       "      <th>R31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### R41\n",
    "\n",
    "0. bad\n",
    "1. good\n",
    "2. bad\n",
    "3. good\n",
    "4. good\n",
    "5. good\n",
    "6. good\n",
    "7. good\n",
    "8. good\n",
    "9. good\n",
    "10. good\n",
    "11. good\n",
    "12. good\n",
    "13. good\n",
    "14. bad\n",
    "15. good\n",
    "16. good\n",
    "17. good\n",
    "18. good\n",
    "19. good\n",
    "\n",
    "--> 17 points"
   ],
   "id": "579ae0e96c572ae6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T14:01:16.187217Z",
     "start_time": "2024-05-13T14:01:16.171475Z"
    }
   },
   "cell_type": "code",
   "source": "scores_41[\"Mean\"]",
   "id": "a1b2fb5322d36cfd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -1.0\n",
       "1    -1.0\n",
       "2    -1.0\n",
       "3    -1.0\n",
       "4    -1.0\n",
       "5    -1.0\n",
       "6     0.0\n",
       "7    -1.0\n",
       "8    -1.0\n",
       "9    -1.0\n",
       "10    0.0\n",
       "11   -1.0\n",
       "12   -1.0\n",
       "13   -1.0\n",
       "14   -0.8\n",
       "15   -1.0\n",
       "16    0.0\n",
       "17   -1.0\n",
       "18    0.0\n",
       "19   -0.6\n",
       "Name: Mean, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion:\n",
    "\n",
    "Prompt3 with gpt-4-turbo, top_p = 1, temp = 0 is the best model for this task. Anonymizing data has no effect on the quality of the sentiment evaluation. It outperforms the other models in terms of consistency and quality.\n",
    "\n",
    "Could also take top_p = 0 and temp = 0 but it is not recommended and since it doesnt make a difference in this case, i will go with top_p = 1 and temp = 0."
   ],
   "id": "bee1dbb087008a23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "... könnte noch schauen ob top_p = 0 und temp = 0 vs top_p = 1 und temp = 0 unterschiedlich bezüglich der erklärung sind. Aber ist mir eigentlich schnurz. Hab die erklärung ursprünglich einfach zur kontrolle gebraucht. ",
   "id": "4305eb66453e9f53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Tests R6: \n",
   "id": "558c055a76b97755"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:45:25.955419Z",
     "start_time": "2024-05-14T07:45:25.904293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names_61 = ['Output_Testdata/CS/sample_data_20_20240514_093529_R6-1-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_093625_R6-1-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_093724_R6-1-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_093817_R6-1-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_093914_R6-1-5.csv']\n",
    "dataframes_61 = [pd.read_csv(file_name) for file_name in file_names_61]\n",
    "\n",
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df_61 in dataframes_61:\n",
    "    df_61['Extracted_Score'] = df_61['Sentiment'].apply(extract_score)\n",
    "    \n",
    "\n",
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores_61 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_61['Extracted_Score'] for i, df_61 in enumerate(dataframes_61)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores_61['Mean'] = scores_61.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores_61['Identical Score Percentage'] = scores_61.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(scores_61.head(20))"
   ],
   "id": "9bb53781eb731cd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "1   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "2    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "3    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "4   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "5   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "6    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "7   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "8    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "9   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "10   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "11  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "12  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "13  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "14   0.0   0.0   1.0   0.0   1.0   0.4                        60.0\n",
      "15  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "16   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "17  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "18   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "19  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "70755149eb298672"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# R6-2\n",
   "id": "5eaa56108114490f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:08:18.690125Z",
     "start_time": "2024-05-14T08:08:18.632955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming the files are named run1.csv, run2.csv, etc.\n",
    "file_names_62 = ['Output_Testdata/CS/sample_data_20_20240514_095936_R6-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_100229_R6-2-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_100322_R6-2-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_100422_R6-2-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_100520_R6-2-5.csv']\n",
    "dataframes_62 = [pd.read_csv(file_name) for file_name in file_names_62]\n",
    "\n",
    "def extract_score(sentiment_text):\n",
    "    match = re.match(r\"(-?\\d\\.?\\d*)\\.\", sentiment_text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "for df_62 in dataframes_62:\n",
    "    df_62['Extracted_Score'] = df_62['Sentiment'].apply(extract_score)\n",
    "    \n",
    "\n",
    "# Create a DataFrame to hold the extracted scores from all runs\n",
    "scores_62 = pd.DataFrame({\n",
    "    f'Run{i+1}': df_62['Extracted_Score'] for i, df_62 in enumerate(dataframes_62)\n",
    "})\n",
    "\n",
    "# Calculate mean and standard deviation across runs for each article\n",
    "scores_62['Mean'] = scores_62.mean(axis=1)\n",
    "#scores['StdDev'] = scores.std(axis=1)\n",
    "\n",
    "# Function to calculate the percentage of identical values only considering the first 5 columns (run results)\n",
    "def identical_percentage(row):\n",
    "    # Isolate the first five columns which are the run results\n",
    "    data = row[:5]\n",
    "    unique_values = set(data.dropna())  # Get unique values excluding NaN\n",
    "    if len(unique_values) == 1:\n",
    "        return 100.0  # All values are identical\n",
    "    else:\n",
    "        # Count occurrences of the most common value\n",
    "        most_common = max(set(data), key=list(data).count)\n",
    "        count_most_common = list(data).count(most_common)\n",
    "        return (count_most_common / len(data.dropna())) * 100\n",
    "\n",
    "# Apply the function to each row and store the results in a new column\n",
    "scores_62['Identical Score Percentage'] = scores_62.iloc[:, :5].apply(identical_percentage, axis=1)\n",
    "\n",
    "# Show the first 20 rows to inspect mean, standard deviation, and identical score percentage\n",
    "print(scores_62.head(20))"
   ],
   "id": "299bf75e89416f09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run1  Run2  Run3  Run4  Run5  Mean  Identical Score Percentage\n",
      "0    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "1   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "2    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "3    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "4   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "5   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "6    1.0   1.0   1.0   1.0   1.0   1.0                       100.0\n",
      "7   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "8    0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "9   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "10   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "11  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "12  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "13  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "14   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "15  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "16   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "17  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n",
      "18   0.0   0.0   0.0   0.0   0.0   0.0                       100.0\n",
      "19  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0                       100.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### R2-2 vs R6-2",
   "id": "92d58bd643006d93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:30:10.498605Z",
     "start_time": "2024-05-14T08:30:10.455858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_names = ['Output_Testdata/CS/sample_data_20_20240512_180005_R2-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_180005_R2-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_180323_R2-2-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_180432_R2-2-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240512_180522_R2-2-5.csv']\n",
    "\n",
    "dataframes = []\n",
    "for i, file_name in enumerate(file_names):\n",
    "    df = pd.read_csv(file_name)\n",
    "    # Assume 'Sentiment' is the column to be extracted\n",
    "    df = df[['Sentiment']].rename(columns={'Sentiment': f'Run{i+1}'})\n",
    "    dataframes.append(df)"
   ],
   "id": "bf087510de3d4125",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:30:45.189918Z",
     "start_time": "2024-05-14T08:30:45.178923Z"
    }
   },
   "cell_type": "code",
   "source": "dataframes[0]",
   "id": "9a9f13979657cfad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 Run1\n",
       "0   1. Positive earnings report and strategic rest...\n",
       "1   -1. Article highlights failures in risk manage...\n",
       "2   0. The article neutrally presents Credit Suiss...\n",
       "3   -1. Focus on leadership turmoil and spying sca...\n",
       "4   -1. Article highlights significant losses and ...\n",
       "5   -1. Article highlights Credit Suisse's trouble...\n",
       "6   1. Credit Suisse is portrayed positively for p...\n",
       "7   -1. Article highlights Credit Suisse's legal s...\n",
       "8   -1. Article mentions Credit Suisse's involveme...\n",
       "9   -1. Article highlights ongoing struggles, inve...\n",
       "10  -1. Article highlights leadership failures and...\n",
       "11  -1. Article highlights challenges, leadership ...\n",
       "12  -1. Article highlights ongoing issues and inve...\n",
       "13  -1. Article portrays Credit Suisse negatively,...\n",
       "14  1. Positive portrayal of CSX's digital initiat...\n",
       "15  -1. Article highlights negative issues like su...\n",
       "16  -1. Focus on crisis management and negative im...\n",
       "17  -1. Article highlights critical situation and ...\n",
       "18  1. Positive outlook on leadership and strategi...\n",
       "19  -1. Concerns about internal processes and inef..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Positive earnings report and strategic rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1. Article highlights failures in risk manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0. The article neutrally presents Credit Suiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1. Focus on leadership turmoil and spying sca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1. Article highlights significant losses and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1. Article highlights Credit Suisse's trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1. Credit Suisse is portrayed positively for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1. Article highlights Credit Suisse's legal s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1. Article mentions Credit Suisse's involveme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1. Article highlights ongoing struggles, inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1. Article highlights leadership failures and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1. Article highlights challenges, leadership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1. Article highlights ongoing issues and inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1. Article portrays Credit Suisse negatively,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1. Positive portrayal of CSX's digital initiat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1. Article highlights negative issues like su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1. Focus on crisis management and negative im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1. Article highlights critical situation and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1. Positive outlook on leadership and strategi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1. Concerns about internal processes and inef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:35:49.457514Z",
     "start_time": "2024-05-14T08:35:49.427148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_names = ['Output_Testdata/CS/sample_data_20_20240514_095936_R6-2-1.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_100229_R6-2-2.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_100322_R6-2-3.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_100422_R6-2-4.csv',\n",
    "              'Output_Testdata/CS/sample_data_20_20240514_100520_R6-2-5.csv']\n",
    "\n",
    "dataframes = []\n",
    "for i, file_name in enumerate(file_names):\n",
    "    df = pd.read_csv(file_name)\n",
    "    # Assume 'Sentiment' is the column to be extracted\n",
    "    df = df[['Sentiment']].rename(columns={'Sentiment': f'Run{i+1}'})\n",
    "    dataframes.append(df)"
   ],
   "id": "84d46c44143f3ae3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:35:56.088918Z",
     "start_time": "2024-05-14T08:35:56.075460Z"
    }
   },
   "cell_type": "code",
   "source": "dataframes[1]",
   "id": "8579a58b80437cdc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 Run2\n",
       "0   1. Positive financial performance and strategi...\n",
       "1   -1. The article highlights significant failure...\n",
       "2   0. The article presents Credit Suisse's analys...\n",
       "3   0. Mixed reactions to leadership changes and s...\n",
       "4   -1. The article highlights significant losses ...\n",
       "5   -1. The article highlights Credit Suisse's for...\n",
       "6   1. Credit Suisse is portrayed positively for i...\n",
       "7   -1. The article highlights ongoing legal troub...\n",
       "8   0. Credit Suisse is mentioned factually withou...\n",
       "9   -1. The article highlights ongoing struggles, ...\n",
       "10  0. The sentiment is neutral, focusing on factu...\n",
       "11  -1. The article highlights crises, leadership ...\n",
       "12  -1. The article highlights ongoing issues, inv...\n",
       "13  -1. The article highlights negative aspects an...\n",
       "14  0. The sentiment is neutral, providing a balan...\n",
       "15  -1. The article highlights a scandal and ongoi...\n",
       "16  0. The sentiment is neutral as the article dis...\n",
       "17  -1. The article highlights critical issues and...\n",
       "18  0. The sentiment is neutral, focusing on factu...\n",
       "19  -1. The article highlights failures and challe..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Positive financial performance and strategi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1. The article highlights significant failure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0. The article presents Credit Suisse's analys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0. Mixed reactions to leadership changes and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1. The article highlights significant losses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1. The article highlights Credit Suisse's for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1. Credit Suisse is portrayed positively for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1. The article highlights ongoing legal troub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0. Credit Suisse is mentioned factually withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1. The article highlights ongoing struggles, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0. The sentiment is neutral, focusing on factu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1. The article highlights crises, leadership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1. The article highlights ongoing issues, inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1. The article highlights negative aspects an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0. The sentiment is neutral, providing a balan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1. The article highlights a scandal and ongoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0. The sentiment is neutral as the article dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1. The article highlights critical issues and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0. The sentiment is neutral, focusing on factu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1. The article highlights failures and challe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
